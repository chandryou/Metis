@article{ruder_overview_2017,
	title = {An {Overview} of {Multi}-{Task} {Learning} in {Deep} {Neural} {Networks}},
	url = {https://arxiv.org/abs/1706.05098v1},
	abstract = {Multi-task learning (MTL) has led to successes in many applications of
machine learning, from natural language processing and speech recognition to
computer vision and drug discovery. This article aims to give a general
overview of MTL, particularly in deep neural networks. It introduces the two
most common methods for MTL in Deep Learning, gives an overview of the
literature, and discusses recent advances. In particular, it seeks to help ML
practitioners apply MTL by shedding light on how MTL works and providing
guidelines for choosing appropriate auxiliary tasks.},
	language = {en},
	urldate = {2020-07-24},
	author = {Ruder, Sebastian},
	month = jun,
	year = {2017},
	file = {Full Text PDF:/Users/chan/Zotero/storage/F7P8FLFV/Ruder - 2017 - An Overview of Multi-Task Learning in Deep Neural .pdf:application/pdf;Snapshot:/Users/chan/Zotero/storage/X8PA9J49/1706.html:text/html}
}

@article{ramsundar_massively_2015,
	title = {Massively {Multitask} {Networks} for {Drug} {Discovery}},
	url = {http://arxiv.org/abs/1502.02072},
	abstract = {Massively multitask neural architectures provide a learning framework for drug discovery that synthesizes information from many distinct biological sources. To train these architectures at scale, we gather large amounts of data from public sources to create a dataset of nearly 40 million measurements across more than 200 biological targets. We investigate several aspects of the multitask framework by performing a series of empirical studies and obtain some interesting results: (1) massively multitask networks obtain predictive accuracies significantly better than single-task methods, (2) the predictive power of multitask networks improves as additional tasks and data are added, (3) the total amount of data and the total number of tasks both contribute significantly to multitask improvement, and (4) multitask networks afford limited transferability to tasks not in the training set. Our results underscore the need for greater data sharing and further algorithmic innovation to accelerate the drug discovery process.},
	urldate = {2020-07-24},
	journal = {arXiv:1502.02072 [cs, stat]},
	author = {Ramsundar, Bharath and Kearnes, Steven and Riley, Patrick and Webster, Dale and Konerding, David and Pande, Vijay},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.02072},
	keywords = {Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning},
	annote = {Comment: Preliminary work. Under review by the International Conference on Machine Learning (ICML)},
	file = {arXiv Fulltext PDF:/Users/chan/Zotero/storage/6THSCKWR/Ramsundar et al. - 2015 - Massively Multitask Networks for Drug Discovery.pdf:application/pdf;arXiv.org Snapshot:/Users/chan/Zotero/storage/XGLAEC2E/1502.html:text/html}
}

@article{bengio_meta-transfer_2019,
	title = {A {Meta}-{Transfer} {Objective} for {Learning} to {Disentangle} {Causal} {Mechanisms}},
	url = {http://arxiv.org/abs/1901.10912},
	abstract = {We propose to meta-learn causal structures based on how fast a learner adapts to new distributions arising from sparse distributional changes, e.g. due to interventions, actions of agents and other sources of non-stationarities. We show that under this assumption, the correct causal structural choices lead to faster adaptation to modified distributions because the changes are concentrated in one or just a few mechanisms when the learned knowledge is modularized appropriately. This leads to sparse expected gradients and a lower effective number of degrees of freedom needing to be relearned while adapting to the change. It motivates using the speed of adaptation to a modified distribution as a meta-learning objective. We demonstrate how this can be used to determine the cause-effect relationship between two observed variables. The distributional changes do not need to correspond to standard interventions (clamping a variable), and the learner has no direct knowledge of these interventions. We show that causal structures can be parameterized via continuous variables and learned end-to-end. We then explore how these ideas could be used to also learn an encoder that would map low-level observed variables to unobserved causal variables leading to faster adaptation out-of-distribution, learning a representation space where one can satisfy the assumptions of independent mechanisms and of small and sparse changes in these mechanisms due to actions and non-stationarities.},
	urldate = {2020-07-24},
	journal = {arXiv:1901.10912 [cs, stat]},
	author = {Bengio, Yoshua and Deleu, Tristan and Rahaman, Nasim and Ke, Rosemary and Lachapelle, SÃ©bastien and Bilaniuk, Olexa and Goyal, Anirudh and Pal, Christopher},
	month = feb,
	year = {2019},
	note = {arXiv: 1901.10912},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/chan/Zotero/storage/MP4G64FM/Bengio et al. - 2019 - A Meta-Transfer Objective for Learning to Disentan.pdf:application/pdf;arXiv.org Snapshot:/Users/chan/Zotero/storage/9G8W56YY/1901.html:text/html}
}
